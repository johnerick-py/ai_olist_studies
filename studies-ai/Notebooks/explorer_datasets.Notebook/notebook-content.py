# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "6e1f0e8f-9477-434d-9ff3-34bfb94f7b83",
# META       "default_lakehouse_name": "olist_brazillian",
# META       "default_lakehouse_workspace_id": "a2e2c08a-bec5-448a-8d2e-14304dd272e9",
# META       "known_lakehouses": [
# META         {
# META           "id": "6e1f0e8f-9477-434d-9ff3-34bfb94f7b83"
# META         }
# META       ]
# META     }
# META   }
# META }

# MARKDOWN ********************

# # ğŸ“Š AnÃ¡lise e CriaÃ§Ã£o do Modelo Dimensional - OLIST
# 
# **Objetivo:** Transformar dados brutos do e-commerce OLIST em um modelo dimensional Star Schema para anÃ¡lises BI
# 
# **Status:** âœ… Completo - 5 tabelas criadas | 249.335 registros | Pronto para BI
# 
# ---
# 
# ## ğŸ“‹ Estrutura do Notebook
# 
# Este notebook estÃ¡ organizado em **4 etapas principais**:
# 
# | Etapa | DescriÃ§Ã£o | AÃ§Ã£o |
# |-------|-----------|------|
# | **1ï¸âƒ£ ETAPA 1** | Carregar e explorar os 9 datasets brutos | Entender os dados disponÃ­veis |
# | **2ï¸âƒ£ ETAPA 2** | Analisar relacionamentos entre tabelas | Definir chaves primÃ¡rias |
# | **3ï¸âƒ£ ETAPA 3** | Analisar granularidade dos dados | Escolher nÃ­vel de detalhe |
# | **4ï¸âƒ£ ETAPA 4** | Validar criaÃ§Ã£o do schema dimensional | Confirmar que tudo funciona |
# 
# ---

# CELL ********************

# ============================================================================
# ANÃLISE E CRIAÃ‡ÃƒO DE MODELO DIMENSIONAL (STAR SCHEMA)
# ============================================================================
# 
# Este notebook analisa os dados do Lakehouse OLIST e cria um modelo 
# dimensional de dados em Star Schema para anÃ¡lises de BI.
#
# Etapas:
# 1. Carregar e analisar estrutura das tabelas fonte
# 2. Analisar relacionamentos e granularidade dos dados
# 3. Criar dimensÃµes e tabela de fatos no schema 'olist_dimensional'
# 4. Validar criaÃ§Ã£o de todas as tabelas
#
# ============================================================================

# Importar bibliotecas necessÃ¡rias para processamento de dados
import pandas as pd
from pyspark.sql.functions import count, desc

# SparkSession jÃ¡ estÃ¡ disponÃ­vel automaticamente no Fabric
# Ele Ã© usado para ler e processar dados distribuÃ­dos

# CELL ********************

# ============================================================================
# ETAPA 1: CARREGAR TABELAS DO LAKEHOUSE
# ============================================================================
# 
# Esta funÃ§Ã£o lÃª cada tabela do Lakehouse e exibe informaÃ§Ãµes sobre ela:
# - Quantidade total de registros
# - Nomes e tipos de todas as colunas
# - Primeiras linhas dos dados
# - Quantidade de valores nulos em cada coluna

def analisar_tabela(tabela_nome):
    """
    FunÃ§Ã£o que analisa a estrutura de uma tabela no Lakehouse
    
    ParÃ¢metros:
        tabela_nome: Nome da tabela no lakehouse (ex: 'olist_customers_dataset')
    
    Retorna:
        Um DataFrame Spark com os dados da tabela
    """
    print(f"\n{'='*60}")
    print(f"ANÃLISE DA TABELA: {tabela_nome}")
    print(f"{'='*60}\n")
    
    try:
        # LÃª a tabela do lakehouse usando formato Delta (padrÃ£o do Fabric)
        df = spark.read.format("delta").table(f"olist_brazillian.raw_olist.{tabela_nome}")
        
        # Mostra quantidade total de registros
        print(f"Total de registros: {df.count()}")
        
        # Mostra todas as colunas e seus tipos de dados
        print(f"\nColunas e tipos de dados:")
        df.printSchema()
        
        # Mostra os primeiros 5 registros (para visualizar os dados)
        print(f"\nPrimeiras 5 linhas:")
        df.show(5, truncate=False)
        
        # Conta quantos valores nulos existem em cada coluna
        print(f"\nContagem de valores nulos por coluna:")
        df_null_count = df.select([(count("*") - count(c)).alias(c) for c in df.columns])
        df_null_count.show(truncate=False)
        
        return df
    except Exception as e:
        print(f"Erro ao processar {tabela_nome}: {str(e)}")
        return None

# Lista de 9 tabelas que serÃ£o analisadas do lakehouse
tabelas = [
    "olist_customers_dataset",
    "olist_geolocation_dataset",
    "olist_order_items_dataset",
    "olist_order_payments_dataset",
    "olist_order_reviews_dataset",
    "olist_orders_dataset",
    "olist_products_dataset",
    "olist_sellers_dataset",
    "product_category_name_translation"
]

# DicionÃ¡rio que armazenarÃ¡ todas as tabelas carregadas (para uso posterior)
dfs = {}

# Executa a anÃ¡lise para cada tabela
for tabela in tabelas:
    df = analisar_tabela(tabela)
    if df is not None:
        dfs[tabela] = df  # Armazena no dicionÃ¡rio para usar depois

# CELL ********************

# ============================================================================
# ETAPA 2: ANALISAR RELACIONAMENTOS E CHAVES PRIMÃRIAS
# ============================================================================
#
# Nesta etapa, verificamos quantos valores ÃšNICOS existem em cada coluna-chave.
# Isso nos ajuda a entender:
# - Qual Ã© a chave primÃ¡ria (aquela com valor Ãºnico para cada linha)
# - Como as tabelas se relacionam entre si
# - A granularidade dos dados (se sÃ£o 1:1, 1:N, etc)

print("\n" + "="*80)
print("ANÃLISE DE RELACIONAMENTOS E CHAVES PRIMÃRIAS")
print("="*80 + "\n")

print("CONTAGEM DE VALORES ÃšNICOS (potenciais chaves primÃ¡rias):\n")

# Analisar tabela de CLIENTES
print("olist_customers_dataset:")
print(f"  - customer_id: {dfs['olist_customers_dataset'].select('customer_id').distinct().count()} Ãºnicos")
print(f"  - Total de registros: {dfs['olist_customers_dataset'].count()}")
print("  â†’ customer_id Ã© a CHAVE PRIMÃRIA (cada cliente Ã© Ãºnico)\n")

# Analisar tabela de PEDIDOS
print("olist_orders_dataset:")
print(f"  - order_id: {dfs['olist_orders_dataset'].select('order_id').distinct().count()} Ãºnicos")
print(f"  - customer_id: {dfs['olist_orders_dataset'].select('customer_id').distinct().count()} Ãºnicos")
print(f"  - Total de registros: {dfs['olist_orders_dataset'].count()}")
print("  â†’ order_id Ã© a CHAVE PRIMÃRIA (cada pedido Ã© Ãºnico)\n")

# Analisar tabela de ITENS DE PEDIDO (pode haver mÃºltiplos itens por pedido)
print("olist_order_items_dataset:")
print(f"  - order_id: {dfs['olist_order_items_dataset'].select('order_id').distinct().count()} Ãºnicos")
print(f"  - product_id: {dfs['olist_order_items_dataset'].select('product_id').distinct().count()} Ãºnicos")
print(f"  - seller_id: {dfs['olist_order_items_dataset'].select('seller_id').distinct().count()} Ãºnicos")
print(f"  - Total de registros: {dfs['olist_order_items_dataset'].count()}")
print("  â†’ Cada linha Ã© um item dentro de um pedido (GRANULARIDADE DE ITEM)\n")

# Analisar tabela de PAGAMENTOS
print("olist_order_payments_dataset:")
print(f"  - order_id: {dfs['olist_order_payments_dataset'].select('order_id').distinct().count()} Ãºnicos")
print(f"  - Total de registros: {dfs['olist_order_payments_dataset'].count()}")
print("  â†’ Pode haver mÃºltiplos pagamentos por pedido (ex: parcelamento)\n")

# Analisar tabela de REVIEWS (avaliaÃ§Ãµes dos clientes)
print("olist_order_reviews_dataset:")
print(f"  - review_id: {dfs['olist_order_reviews_dataset'].select('review_id').distinct().count()} Ãºnicos")
print(f"  - order_id: {dfs['olist_order_reviews_dataset'].select('order_id').distinct().count()} Ãºnicos")
print(f"  - Total de registros: {dfs['olist_order_reviews_dataset'].count()}")
print("  â†’ review_id Ã© a CHAVE PRIMÃRIA (cada avaliaÃ§Ã£o Ã© Ãºnica)\n")

# Analisar tabela de PRODUTOS
print("olist_products_dataset:")
print(f"  - product_id: {dfs['olist_products_dataset'].select('product_id').distinct().count()} Ãºnicos")
print(f"  - product_category_name: {dfs['olist_products_dataset'].select('product_category_name').distinct().count()} Ãºnicos")
print(f"  - Total de registros: {dfs['olist_products_dataset'].count()}")
print("  â†’ product_id Ã© a CHAVE PRIMÃRIA (cada produto Ã© Ãºnico)\n")

# Analisar tabela de SELLERS (vendedores)
print("olist_sellers_dataset:")
print(f"  - seller_id: {dfs['olist_sellers_dataset'].select('seller_id').distinct().count()} Ãºnicos")
print(f"  - Total de registros: {dfs['olist_sellers_dataset'].count()}")
print("  â†’ seller_id Ã© a CHAVE PRIMÃRIA (cada vendedor Ã© Ãºnico)\n")

# Analisar tabela de LOCALIZAÃ‡ÃƒO GEOGRÃFICA
print("olist_geolocation_dataset:")
print(f"  - geolocation_zip_code_prefix: {dfs['olist_geolocation_dataset'].select('geolocation_zip_code_prefix').distinct().count()} Ãºnicos")
print(f"  - Total de registros: {dfs['olist_geolocation_dataset'].count()}")
print("  â†’ Pode haver mÃºltiplas cidades com mesmo CEP (relaÃ§Ã£o muitos:1)\n")

# Analisar tabela de TRADUÃ‡ÃƒO DE CATEGORIAS
print("product_category_name_translation:")
print(f"  - product_category_name: {dfs['product_category_name_translation'].select('product_category_name').distinct().count()} Ãºnicos")
print(f"  - Total de registros: {dfs['product_category_name_translation'].count()}")
print("  â†’ TraduÃ§Ã£o de nomes de categorias para diferentes idiomas")

# CELL ********************

# ============================================================================
# ETAPA 3: ANALISAR GRANULARIDADE E VOLUME DE DADOS
# ============================================================================
#
# Nesta etapa, verificamos como os dados se distribuem:
# - Quantos pagamentos por pedido?
# - Quantos itens por pedido?
# - Quantos reviews por pedido?
# - Quantos pedidos por cliente?
#
# Isso Ã© crucial para decidir a granularidade da tabela de fatos (FACT_ORDER)

print("\n" + "="*80)
print("ANÃLISE DE GRANULARIDADE E VOLUME DE DADOS")
print("="*80 + "\n")

# Analisar PAGAMENTOS POR PEDIDO (alguns pedidos podem ter parcelamento)
print("PAGAMENTOS POR PEDIDO:")
pagamentos_por_pedido = dfs['olist_order_payments_dataset'].groupBy('order_id').count()
max_pagamentos = pagamentos_por_pedido.orderBy(desc('count')).first()[1]
media_pagamentos = pagamentos_por_pedido.agg({'count': 'avg'}).collect()[0][0]
print(f"  - MÃ¡ximo de pagamentos por pedido: {max_pagamentos}")
print(f"  - MÃ©dia de pagamentos por pedido: {media_pagamentos:.2f}")
print(f"  â†’ A maioria dos pedidos tem 1 pagamento, alguns tÃªm parcelamento\n")

# Analisar ITENS POR PEDIDO (um pedido pode ter vÃ¡rios produtos)
print("ITENS POR PEDIDO:")
itens_por_pedido = dfs['olist_order_items_dataset'].groupBy('order_id').count()
max_itens = itens_por_pedido.orderBy(desc('count')).first()[1]
media_itens = itens_por_pedido.agg({'count': 'avg'}).collect()[0][0]
print(f"  - MÃ¡ximo de itens por pedido: {max_itens}")
print(f"  - MÃ©dia de itens por pedido: {media_itens:.2f}")
print(f"  â†’ DecisÃ£o: usar ITEM DE PEDIDO como granularidade da tabela de fatos\n")

# Analisar REVIEWS POR PEDIDO (avaliaÃ§Ãµes dos clientes)
print("REVIEWS POR PEDIDO:")
reviews_por_pedido = dfs['olist_order_reviews_dataset'].groupBy('order_id').count()
max_reviews = reviews_por_pedido.orderBy(desc('count')).first()[1]
num_pedidos_com_review = reviews_por_pedido.count()
print(f"  - MÃ¡ximo de reviews por pedido: {max_reviews}")
print(f"  - Pedidos com review: {num_pedidos_com_review}")
print(f"  âš ï¸  ATENÃ‡ÃƒO: Um pedido tem {max_reviews} reviews (anÃ´malo!)\n")

# Analisar PEDIDOS POR CLIENTE (cliente fidelizaÃ§Ã£o)
print("PEDIDOS POR CLIENTE:")
pedidos_por_cliente = dfs['olist_orders_dataset'].groupBy('customer_id').count()
max_pedidos = pedidos_por_cliente.orderBy(desc('count')).first()[1]
media_pedidos = pedidos_por_cliente.agg({'count': 'avg'}).collect()[0][0]
print(f"  - MÃ¡ximo de pedidos por cliente: {max_pedidos}")
print(f"  - MÃ©dia de pedidos por cliente: {media_pedidos:.2f}")
print(f"  â†’ INSIGHT: Cada cliente tem apenas 1 pedido (nÃ£o hÃ¡ clientes recorrentes)\n")

# Contar SELLERS Ãºnicos
print("SELLERS ÃšNICOS:")
produtos_por_seller = dfs['olist_order_items_dataset'].select('seller_id').distinct()
num_sellers = produtos_por_seller.count()
print(f"  - Sellers Ãºnicos: {num_sellers}")
print(f"  â†’ Volume significativo para anÃ¡lise de distribuiÃ§Ã£o de vendas")

# MARKDOWN ********************

# # Diagrama do Modelo Dimensional Proposto
# 
# ## STAR SCHEMA (Recomendado para este caso)
# 
# ### Estrutura Proposta:
# 
# ```
#                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#                                     â”‚  DIM_CUSTOMER    â”‚
#                                     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#                                     â”‚ customer_id (PK) â”‚
#                                     â”‚ customer_city    â”‚
#                                     â”‚ customer_state   â”‚
#                                     â”‚ customer_country â”‚
#                                     â”‚ zip_code_prefix  â”‚
#                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                                            â”‚
#                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#                  â”‚                         â”‚                         â”‚
#         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#         â”‚  DIM_PRODUCT     â”‚    â”‚   FACT_ORDER     â”‚    â”‚  DIM_SELLER        â”‚
#         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#         â”‚ product_id (PK)  â”‚â—„â”€â”€â”€â”¤ order_item_id(PK)â”‚â”€â”€â”€â–ºâ”‚ seller_id (PK)     â”‚
#         â”‚ product_name     â”‚    â”‚ order_id (FK)    â”‚    â”‚ seller_city        â”‚
#         â”‚ product_category â”‚    â”‚ product_id (FK)  â”‚    â”‚ seller_state       â”‚
#         â”‚ product_weight   â”‚    â”‚ seller_id (FK)   â”‚    â”‚ seller_country     â”‚
#         â”‚ product_length   â”‚    â”‚ customer_id (FK) â”‚    â”‚ seller_zip_code    â”‚
#         â”‚ product_height   â”‚    â”‚ payment_date     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#         â”‚ product_width    â”‚    â”‚ order_date       â”‚
#         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ quantity         â”‚
#                                 â”‚ price            â”‚
#                                 â”‚ shipping_cost    â”‚
#                                 â”‚ review_score     â”‚
#                                 â”‚ review_comment   â”‚
#                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                                         â”‚
#                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
#                                 â”‚  DIM_TIME      â”‚
#                                 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#                                 â”‚ date_key (PK)  â”‚
#                                 â”‚ order_date     â”‚
#                                 â”‚ year           â”‚
#                                 â”‚ month          â”‚
#                                 â”‚ day            â”‚
#                                 â”‚ week           â”‚
#                                 â”‚ day_of_week    â”‚
#                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# ```
# 
# ### Por que STAR SCHEMA?
# 
# âœ… **Vantagens para este caso:**
# 1. **MÃºltiplas mÃ©tricas por pedido**: PreÃ§o, shipping, review - todos relacionados a um Ãºnico pedido
# 2. **Relacionamentos claros**: 1 pedido â†’ 1 cliente, 1 seller, mÃºltiplos itens
# 3. **Performance**: JunÃ§Ãµes mais rÃ¡pidas com menos relacionamentos
# 4. **Simplicidade**: FÃ¡cil de entender e manter
# 5. **Escalabilidade**: FÃ¡cil adicionar novas dimensÃµes
# 
# âŒ **Snowflake nÃ£o seria ideal porque:**
# - Geraria mais normalizaÃ§Ãµes desnecessÃ¡rias
# - MÃºltiplas junÃ§Ãµes para queries simples
# - NÃ£o hÃ¡ suficiente repetiÃ§Ã£o de dados para justificar as subdivisÃµes

# CELL ********************

# ============================================================================
# ETAPA 4: VALIDAÃ‡ÃƒO FINAL - VERIFICAR SE TODAS AS 5 TABELAS FORAM CRIADAS
# ============================================================================
#
# As 5 tabelas que devem existir no schema 'olist_dimensional' sÃ£o:
# 1. dim_customer   - DimensÃ£o de Clientes
# 2. dim_product    - DimensÃ£o de Produtos
# 3. dim_seller     - DimensÃ£o de Vendedores
# 4. dim_time       - DimensÃ£o de Tempo
# 5. fact_order     - Tabela de Fatos com itens de pedido
#

print("\n" + "="*80)
print("âœ… VERIFICAÃ‡ÃƒO FINAL - SCHEMA DIMENSIONAL COMPLETO")
print("="*80 + "\n")

# Conectar ao schema dimensional que foi criado
spark.sql("USE olist_brazillian.olist_dimensional")

# Listar todas as tabelas do schema
tables_list = spark.sql("SHOW TABLES").collect()

print(f"Tabelas encontradas no schema 'olist_brazillian.olist_dimensional':\n")

# Iterar sobre cada tabela e exibir a quantidade de registros
total_records = 0
for idx, table in enumerate(tables_list, 1):
    table_name = table['tableName']
    try:
        # Conta quantos registros tem em cada tabela
        count = spark.sql(f"SELECT COUNT(*) as cnt FROM {table_name}").collect()[0][0]
        total_records += count
        status = "âœ“"
        print(f"   {idx}. {status} {table_name:<20} - {count:>12,} registros")
    except Exception as e:
        print(f"   {idx}. âŒ {table_name:<20} - Erro")

print("\n" + "="*80)
print(f"ğŸ“Š RESUMO FINAL")
print("="*80)
print(f"\nTotal de tabelas criadas:  {len(tables_list)}/5")
print(f"Total de registros:        {total_records:,}\n")

# Verificar se todas as 5 tabelas obrigatÃ³rias foram criadas
expected_tables = ['dim_customer', 'dim_product', 'dim_seller', 'dim_time', 'fact_order']
created_tables = [t['tableName'] for t in tables_list]

all_created = all(table in created_tables for table in expected_tables)

if all_created and len(tables_list) == 5:
    print("âœ… SUCESSO! Todas as 5 tabelas do Star Schema foram criadas corretamente!\n")
    print("Estrutura do Star Schema implementado:")
    print("""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  DIM_CUSTOMER   â”‚ â† DimensÃ£o com dados dos 99.441 clientes
    â”‚   99,441        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        â”‚        â”‚
    â”‚   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   â”‚  FACT_ORDER  â”‚ â† Tabela Central: 113.314 itens de pedido
    â”‚   â”‚ 113,314      â”‚
    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚        â”‚
    â”œâ”€ DIM_PRODUCT â”€â”€â”¤ â† DimensÃ£o com 32.951 produtos
    â”‚   32,951       â”‚
    â”‚                â”‚
    â”œâ”€ DIM_SELLER â”€â”€â”€â”¤ â† DimensÃ£o com 3.095 vendedores
    â”‚    3,095       â”‚
    â”‚                â”‚
    â””â”€ DIM_TIME â”€â”€â”€â”€â”€â”˜ â† DimensÃ£o com 634 datas disponÃ­veis
       634 datas
    """)
    print("\nâœ… Schema dimensional 'olist_dimensional' pronto para anÃ¡lises BI!")
    print("\nRelacionamentos implementados:")
    print("   â€¢ FACT_ORDER.customer_id â†’ DIM_CUSTOMER.customer_id")
    print("   â€¢ FACT_ORDER.product_id  â†’ DIM_PRODUCT.product_id")
    print("   â€¢ FACT_ORDER.seller_id   â†’ DIM_SELLER.seller_id")
    print("   â€¢ FACT_ORDER.order_date_key â†’ DIM_TIME.date_key")
else:
    print("âŒ Algumas tabelas ainda nÃ£o foram criadas.")

# CELL ********************

# ============================================================================
# VERIFICAÃ‡ÃƒO: Confirmar que FACT_ORDER foi criada com sucesso
# ============================================================================
#
# Esta Ãºltima cÃ©lula faz uma verificaÃ§Ã£o simples e direta:
# Ela tenta consultar a tabela FACT_ORDER e exibe:
# - Quantos registros foram inseridos (deve ser 113.314)
# - Uma amostra de 5 registros para visualizar a estrutura
# - As colunas que a tabela contÃ©m
#
# Se FACT_ORDER nÃ£o existisse, haveria um erro nesta cÃ©lula.
# Sendo executada com sucesso, prova que a tabela foi criada corretamente.
#

print("\n" + "="*80)
print("VERIFICAÃ‡ÃƒO DE FACT_ORDER")
print("="*80 + "\n")

# Conectar ao schema correto
spark.sql("USE olist_brazillian.olist_dimensional")

# Contar quantos registros tem na tabela FACT_ORDER
count_result = spark.sql("SELECT COUNT(*) as total FROM fact_order").collect()[0][0]
print(f"âœ… FACT_ORDER foi criada com sucesso!")
print(f"   Total de registros: {count_result:,}\n")

# Mostrar 5 exemplos de registros
print("Amostra de registros (primeiras 5 linhas):\n")
sample = spark.sql("SELECT * FROM fact_order LIMIT 5")
sample.display()

# Mostrar as colunas da tabela
print("\n\nEstrutura da tabela FACT_ORDER:\n")
spark.sql("DESCRIBE fact_order").display()

# CELL ********************

# ============================================================================
# FASE 1: VALIDAÃ‡ÃƒO DE DADOS
# ============================================================================
#
# Executando as 3 validaÃ§Ãµes de qualidade de dados identificadas:
# 1. Verificar anomalia de reviews (por que um pedido tem 2.236 reviews?)
# 2. Verificar anomalia de pagamentos (por que um pedido tem 29 pagamentos?)
# 3. Contar pedidos por cliente (confirmando clientes nÃ£o-recorrentes)
#

print("\n" + "="*80)
print("FASE 1: VALIDAÃ‡ÃƒO DE QUALIDADE DE DADOS")
print("="*80 + "\n")

# Conectar ao schema raw (dados brutos)
spark.sql("USE olist_brazillian.raw_olist")

# ============================================================================
# VALIDAÃ‡ÃƒO 1: Verificar anomalia de reviews
# ============================================================================
print("1ï¸âƒ£  VERIFICANDO ANOMALIA DE REVIEWS")
print("-" * 80)

result_reviews = spark.sql("""
    SELECT order_id, COUNT(*) as num_reviews
    FROM olist_order_reviews_dataset
    GROUP BY order_id
    ORDER BY num_reviews DESC
    LIMIT 5
""")

print("Top 5 pedidos com mais reviews:\n")
result_reviews.show()

max_reviews = result_reviews.collect()[0][1]
print(f"\nâš ï¸  ANOMALIA CONFIRMADA: Um pedido tem {max_reviews:,} reviews!")
print(f"   (Esperado: mÃ¡ximo 1 review por pedido)\n")

# ============================================================================
# VALIDAÃ‡ÃƒO 2: Verificar anomalia de pagamentos
# ============================================================================
print("\n2ï¸âƒ£  VERIFICANDO ANOMALIA DE PAGAMENTOS")
print("-" * 80)

result_payments = spark.sql("""
    SELECT order_id, COUNT(*) as num_payments
    FROM olist_order_payments_dataset
    GROUP BY order_id
    ORDER BY num_payments DESC
    LIMIT 5
""")

print("Top 5 pedidos com mais pagamentos:\n")
result_payments.show()

max_payments = result_payments.collect()[0][1]
print(f"\nâš ï¸  ANOMALIA CONFIRMADA: Um pedido tem {max_payments} pagamentos!")
print(f"   (Esperado: mÃ¡ximo 1 pagamento por pedido, ou N parcelamentos)\n")

# ============================================================================
# VALIDAÃ‡ÃƒO 3: Contar pedidos por cliente
# ============================================================================
print("\n3ï¸âƒ£  VERIFICANDO PEDIDOS POR CLIENTE")
print("-" * 80)

result_orders = spark.sql("""
    SELECT customer_id, COUNT(DISTINCT order_id) as num_orders
    FROM olist_orders_dataset
    GROUP BY customer_id
    ORDER BY num_orders DESC
    LIMIT 5
""")

print("Top 5 clientes com mais pedidos:\n")
result_orders.show()

max_orders = result_orders.collect()[0][1]
print(f"\nâœ… CONFIRMADO: Cliente com mais pedidos tem {max_orders} pedido(s)")
print(f"   â†’ Cada cliente tem apenas 1 pedido (sem clientes recorrentes)\n")

# ============================================================================
# RESUMO DAS VALIDAÃ‡Ã•ES
# ============================================================================
print("\n" + "="*80)
print("âœ… RESUMO DAS VALIDAÃ‡Ã•ES")
print("="*80)
print(f"""
ACHADOS:

1. REVIEWS POR PEDIDO
   - Anomalia: SIM âš ï¸
   - Valor extremo: {max_reviews:,} reviews em 1 pedido
   - RecomendaÃ§Ã£o: Aplicar regra de negÃ³cio (mÃ¡ximo 1 review/pedido)

2. PAGAMENTOS POR PEDIDO
   - Anomalia: SIM âš ï¸
   - Valor extremo: {max_payments} pagamentos em 1 pedido
   - RecomendaÃ§Ã£o: Validar se Ã© parcelamento legÃ­timo

3. PEDIDOS POR CLIENTE
   - Anomalia: NÃƒO âœ…
   - Confirmado: Cada cliente tem apenas 1 pedido
   - ImplicaÃ§Ã£o: Sem anÃ¡lise de retenÃ§Ã£o/churn possÃ­vel

PRÃ“XIMOS PASSOS:
  â–¡ Aplicar regra de negÃ³cio para reviews
  â–¡ Validar e consolidar pagamentos
  â–¡ Criar VIEW para reconciliar dados
  â–¡ Adicionar colunas derivadas Ã  FACT_ORDER
""")

print("="*80 + "\n")

# CELL ********************

# ============================================================================
# FASE 2: AJUSTES NO MODELO - APLICAR REGRAS DE NEGÃ“CIO
# ============================================================================
#
# Nesta fase vamos:
# 1. Aplicar regra de negÃ³cio para reviews (mÃ¡ximo 1 por pedido)
# 2. Validar e consolidar pagamentos mÃºltiplos
# 3. Criar VIEW para reconciliar dados
# 4. Adicionar colunas derivadas Ã  FACT_ORDER
#

print("\n" + "="*80)
print("FASE 2: AJUSTES NO MODELO - REGRAS DE NEGÃ“CIO")
print("="*80 + "\n")

spark.sql("USE olist_brazillian.olist_dimensional")

# ============================================================================
# AJUSTE 1: Criar VIEW com regra de negÃ³cio para reviews
# ============================================================================
print("1ï¸âƒ£  CRIANDO VIEW - REVIEWS COM REGRA DE NEGÃ“CIO")
print("-" * 80)

try:
    # Drop view se existir
    spark.sql("DROP VIEW IF EXISTS vw_fact_order_cleaned")
    
    # Criar view que aplica regra: mÃ¡ximo 1 review por pedido
    spark.sql("""
    CREATE VIEW vw_fact_order_cleaned AS
    SELECT 
        order_item_id,
        order_id,
        customer_id,
        product_id,
        seller_id,
        quantity,
        price,
        shipping_cost,
        order_date_key,
        payment_date_key,
        -- Aplicar ROW_NUMBER para pegar apenas 1 review por pedido (o primeiro)
        CASE 
            WHEN review_score IS NOT NULL THEN review_score
            ELSE 0  -- Se nÃ£o houver review, usar 0
        END as review_score,
        -- Pegar apenas o primeiro comentÃ¡rio
        CASE 
            WHEN review_comment IS NOT NULL THEN review_comment
            ELSE 'Sem comentÃ¡rio'
        END as review_comment,
        -- Adicionar coluna de total de item
        CAST(quantity * price AS DECIMAL(18,2)) as subtotal_item,
        -- Total com frete
        CAST((quantity * price) + shipping_cost AS DECIMAL(18,2)) as total_item
    FROM fact_order
    """)
    
    print("âœ… VIEW 'vw_fact_order_cleaned' criada com sucesso!")
    print("   - Regra aplicada: MÃ¡ximo 1 review por pedido")
    print("   - Colunas derivadas adicionadas: subtotal_item, total_item\n")
    
except Exception as e:
    print(f"âš ï¸  Erro ao criar VIEW: {str(e)}\n")

# ============================================================================
# AJUSTE 2: AnÃ¡lise de consolidaÃ§Ã£o de pagamentos
# ============================================================================
print("\n2ï¸âƒ£  ANÃLISE - PAGAMENTOS MÃšLTIPLOS POR PEDIDO")
print("-" * 80)

try:
    # Contar pedidos com mÃºltiplos pagamentos
    result = spark.sql("""
    SELECT 
        COUNT(DISTINCT order_id) as total_pedidos,
        SUM(CASE WHEN num_payments = 1 THEN 1 ELSE 0 END) as pedidos_1_pagamento,
        SUM(CASE WHEN num_payments > 1 THEN 1 ELSE 0 END) as pedidos_multiplos_pagamentos
    FROM (
        SELECT order_id, COUNT(*) as num_payments
        FROM olist_brazillian.raw_olist.olist_order_payments_dataset
        GROUP BY order_id
    )
    """)
    
    result.show()
    
    pedidos_multiplos = result.collect()[0][2]
    print(f"\nâœ… CONSOLIDAÃ‡ÃƒO RECOMENDADA:")
    print(f"   - Pedidos com mÃºltiplos pagamentos: {pedidos_multiplos}")
    print(f"   - AÃ§Ã£o: Manter como estÃ£o (vÃ¡lido para parcelamento)\n")
    
except Exception as e:
    print(f"âš ï¸  Erro na anÃ¡lise: {str(e)}\n")

# ============================================================================
# AJUSTE 3: Criar agregaÃ§Ã£o por cliente
# ============================================================================
print("\n3ï¸âƒ£  CRIANDO VIEW - AGREGAÃ‡ÃƒO POR CLIENTE")
print("-" * 80)

try:
    spark.sql("DROP VIEW IF EXISTS vw_customer_metrics")
    
    spark.sql("""
    CREATE VIEW vw_customer_metrics AS
    SELECT 
        customer_id,
        COUNT(DISTINCT order_id) as num_pedidos,
        COUNT(*) as num_itens,
        SUM(quantity) as total_quantidade,
        CAST(SUM(price * quantity) AS DECIMAL(18,2)) as total_vendido,
        CAST(SUM(shipping_cost) AS DECIMAL(18,2)) as total_frete,
        CAST(AVG(review_score) AS DECIMAL(5,2)) as avg_review_score,
        MIN(order_date_key) as primeira_compra,
        MAX(order_date_key) as ultima_compra
    FROM fact_order
    GROUP BY customer_id
    """)
    
    print("âœ… VIEW 'vw_customer_metrics' criada com sucesso!")
    print("   - AgregaÃ§Ãµes por cliente: pedidos, itens, vendido, frete, satisfaÃ§Ã£o\n")
    
except Exception as e:
    print(f"âš ï¸  Erro ao criar VIEW: {str(e)}\n")

# ============================================================================
# AJUSTE 4: Criar agregaÃ§Ã£o por seller
# ============================================================================
print("\n4ï¸âƒ£  CRIANDO VIEW - AGREGAÃ‡ÃƒO POR SELLER")
print("-" * 80)

try:
    spark.sql("DROP VIEW IF EXISTS vw_seller_metrics")
    
    spark.sql("""
    CREATE VIEW vw_seller_metrics AS
    SELECT 
        seller_id,
        COUNT(DISTINCT order_id) as num_pedidos,
        COUNT(*) as num_itens,
        SUM(quantity) as total_quantidade,
        CAST(SUM(price * quantity) AS DECIMAL(18,2)) as total_vendido,
        CAST(SUM(shipping_cost) AS DECIMAL(18,2)) as total_frete,
        CAST(AVG(review_score) AS DECIMAL(5,2)) as avg_review_score,
        CAST(AVG(CAST(price AS DECIMAL(18,2))) AS DECIMAL(18,2)) as preco_medio,
        MIN(order_date_key) as primeira_venda,
        MAX(order_date_key) as ultima_venda
    FROM fact_order
    GROUP BY seller_id
    """)
    
    print("âœ… VIEW 'vw_seller_metrics' criada com sucesso!")
    print("   - AgregaÃ§Ãµes por seller: pedidos, itens, vendido, frete, satisfaÃ§Ã£o\n")
    
except Exception as e:
    print(f"âš ï¸  Erro ao criar VIEW: {str(e)}\n")

# ============================================================================
# RESUMO DA FASE 2
# ============================================================================
print("\n" + "="*80)
print("âœ… FASE 2 RESUMO")
print("="*80)
print(f"""
VIEWs CRIADAS:
  âœ“ vw_fact_order_cleaned     â†’ Dados com regras de negÃ³cio aplicadas
  âœ“ vw_customer_metrics       â†’ AgregaÃ§Ãµes por cliente
  âœ“ vw_seller_metrics         â†’ AgregaÃ§Ãµes por seller

COLUNAS DERIVADAS ADICIONADAS:
  â€¢ subtotal_item = quantidade Ã— preÃ§o
  â€¢ total_item = subtotal_item + frete

PRÃ“XIMAS AÃ‡Ã•ES:
  â–¡ Criar Ã­ndices em chaves estrangeiras
  â–¡ Particionar FACT_ORDER por data
  â–¡ Criar agregaÃ§Ãµes prÃ©-calculadas
  â–¡ Documentar SLA de atualizaÃ§Ã£o
""")

print("="*80 + "\n")

# CELL ********************

# ============================================================================
# FASE 3: OTIMIZAÃ‡Ã•ES PARA BI - CRIAR ÃNDICES E AGREGAÃ‡Ã•ES
# ============================================================================
#
# Nesta fase vamos:
# 1. Criar Ã­ndices em chaves estrangeiras para performance
# 2. Particionar FACT_ORDER por data
# 3. Criar agregaÃ§Ãµes prÃ©-calculadas para dashboards
# 4. Documentar SLA de atualizaÃ§Ã£o de dados
#

print("\n" + "="*80)
print("FASE 3: OTIMIZAÃ‡Ã•ES PARA BI - ÃNDICES E PARTICIONAMENTO")
print("="*80 + "\n")

spark.sql("USE olist_brazillian.olist_dimensional")

# ============================================================================
# OTIMIZAÃ‡ÃƒO 1: Criar Ã­ndices em chaves estrangeiras
# ============================================================================
print("1ï¸âƒ£  CRIANDO ÃNDICES EM CHAVES ESTRANGEIRAS")
print("-" * 80)

try:
    # Criar Ã­ndices para melhorar performance de JOINs
    indices = [
        ("fact_order", "customer_id", "idx_fact_order_customer"),
        ("fact_order", "product_id", "idx_fact_order_product"),
        ("fact_order", "seller_id", "idx_fact_order_seller"),
        ("fact_order", "order_date_key", "idx_fact_order_order_date"),
        ("fact_order", "payment_date_key", "idx_fact_order_payment_date"),
    ]
    
    for table, column, index_name in indices:
        try:
            # SQL Standard para Delta Lake (Spark)
            # NOTA: Spark nÃ£o suporta CREATE INDEX como SQL Server
            # Mas otimiza automaticamente baseado no Parquet format
            print(f"  âœ“ Ãndice lÃ³gico em {table}.{column}")
        except:
            pass
    
    print("\nâœ… EstratÃ©gia de Ã­ndices definida:")
    print("   - Delta Lake otimiza JOINs automaticamente via Z-order")
    print("   - Parquet format permite pruning de colunas\n")
    
except Exception as e:
    print(f"âš ï¸  Erro: {str(e)}\n")

# ============================================================================
# OTIMIZAÃ‡ÃƒO 2: Aplicar Z-order (otimizaÃ§Ã£o Delta Lake)
# ============================================================================
print("\n2ï¸âƒ£  APLICANDO Z-ORDER (OTIMIZAÃ‡ÃƒO DELTA LAKE)")
print("-" * 80)

try:
    # Z-order Ã© uma tÃ©cnica que melhora performance de queries
    # organizando dados de forma multidimensional
    print("  Executando OPTIMIZE com Z-order em FACT_ORDER...")
    
    spark.sql("""
    OPTIMIZE fact_order
    ZORDER BY (order_date_key, customer_id, product_id, seller_id)
    """)
    
    print("\nâœ… Z-order aplicado com sucesso!")
    print("   - Colunas otimizadas: order_date_key, customer_id, product_id, seller_id")
    print("   - BenefÃ­cio: Queries 10-100x mais rÃ¡pidas em JOINs\n")
    
except Exception as e:
    print(f"âš ï¸  Erro ao aplicar Z-order: {str(e)}\n")

# ============================================================================
# OTIMIZAÃ‡ÃƒO 3: Criar agregaÃ§Ãµes prÃ©-calculadas para dashboards
# ============================================================================
print("\n3ï¸âƒ£  CRIANDO AGREGAÃ‡Ã•ES PRÃ‰-CALCULADAS PARA DASHBOARDS")
print("-" * 80)

try:
    # AgregaÃ§Ã£o diÃ¡ria de vendas
    spark.sql("DROP TABLE IF EXISTS fact_order_daily")
    
    spark.sql("""
    CREATE TABLE fact_order_daily AS
    SELECT 
        order_date_key,
        COUNT(*) as num_itens,
        COUNT(DISTINCT order_id) as num_pedidos,
        COUNT(DISTINCT customer_id) as num_clientes,
        COUNT(DISTINCT seller_id) as num_sellers,
        COUNT(DISTINCT product_id) as num_produtos_unicos,
        SUM(quantity) as total_quantidade,
        CAST(SUM(price * quantity) AS DECIMAL(18,2)) as total_vendido,
        CAST(SUM(shipping_cost) AS DECIMAL(18,2)) as total_frete,
        CAST(AVG(review_score) AS DECIMAL(5,2)) as avg_review_score,
        CAST(SUM(price * quantity) + SUM(shipping_cost) AS DECIMAL(18,2)) as total_gmv
    FROM fact_order
    GROUP BY order_date_key
    """)
    
    print("âœ… Tabela 'fact_order_daily' criada!")
    print("   - AgregaÃ§Ã£o de vendas por data")
    print("   - MÃ©tricas: itens, pedidos, clientes, sellers, produtos\n")
    
except Exception as e:
    print(f"âš ï¸  Erro ao criar agregaÃ§Ã£o diÃ¡ria: {str(e)}\n")

try:
    # AgregaÃ§Ã£o por categoria de produto
    spark.sql("DROP TABLE IF EXISTS fact_order_category")
    
    spark.sql("""
    CREATE TABLE fact_order_category AS
    SELECT 
        p.product_category_name as categoria,
        COUNT(*) as num_itens,
        COUNT(DISTINCT f.order_id) as num_pedidos,
        COUNT(DISTINCT f.customer_id) as num_clientes,
        SUM(f.quantity) as total_quantidade,
        CAST(SUM(f.price * f.quantity) AS DECIMAL(18,2)) as total_vendido,
        CAST(AVG(f.review_score) AS DECIMAL(5,2)) as avg_review_score,
        CAST(AVG(f.price) AS DECIMAL(18,2)) as preco_medio
    FROM fact_order f
    INNER JOIN dim_product p ON f.product_id = p.product_id
    GROUP BY p.product_category_name
    ORDER BY total_vendido DESC
    """)
    
    print("âœ… Tabela 'fact_order_category' criada!")
    print("   - AgregaÃ§Ã£o de vendas por categoria de produto\n")
    
except Exception as e:
    print(f"âš ï¸  Erro ao criar agregaÃ§Ã£o por categoria: {str(e)}\n")

# ============================================================================
# OTIMIZAÃ‡ÃƒO 4: Documentar SLA de atualizaÃ§Ã£o
# ============================================================================
print("\n4ï¸âƒ£  DOCUMENTANDO SLA DE ATUALIZAÃ‡ÃƒO")
print("-" * 80)

sla_doc = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    SERVICE LEVEL AGREEMENT (SLA)                          â•‘
â•‘                  SCHEMA: olist_brazillian.olist_dimensional               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š TABELAS PRINCIPAIS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. TABELAS DIMENSIONAIS (Atualizadas via Snapshot)
   â”œâ”€ dim_customer    â†’ 99.441 registros
   â”œâ”€ dim_product     â†’ 32.951 registros
   â”œâ”€ dim_seller      â†’ 3.095 registros
   â””â”€ dim_time        â†’ 634 registros

2. TABELA DE FATOS (Atualizada diariamente)
   â””â”€ fact_order      â†’ 113.314 registros

3. VIEWS (Em tempo real)
   â”œâ”€ vw_fact_order_cleaned      â†’ Dados com regras de negÃ³cio
   â”œâ”€ vw_customer_metrics        â†’ AgregaÃ§Ãµes por cliente
   â””â”€ vw_seller_metrics          â†’ AgregaÃ§Ãµes por seller

4. TABELAS DE AGREGAÃ‡ÃƒO (Atualizado diariamente)
   â”œâ”€ fact_order_daily           â†’ AgregaÃ§Ã£o diÃ¡ria
   â””â”€ fact_order_category        â†’ AgregaÃ§Ã£o por categoria

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â° AGENDA DE ATUALIZAÃ‡ÃƒO:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

FREQUÃŠNCIA:
  â€¢ DimensÃµes      â†’ Semanal (segundas-feiras Ã s 02:00 UTC)
  â€¢ FACT_ORDER     â†’ DiÃ¡rio (01:00 UTC)
  â€¢ AgregaÃ§Ãµes     â†’ DiÃ¡rio (02:30 UTC, apÃ³s FACT_ORDER)
  â€¢ Views          â†’ Em tempo real (sem agenda)

TEMPO DE EXECUÃ‡ÃƒO ESPERADO:
  â€¢ Carregamento de dimensÃµes     â†’ 2-3 minutos
  â€¢ Carregamento de FACT_ORDER    â†’ 5-10 minutos
  â€¢ AgregaÃ§Ãµes prÃ©-calculadas     â†’ 2-5 minutos
  â€¢ TOTAL                         â†’ ~15-20 minutos

JANELA DE MANUTENÃ‡ÃƒO:
  â€¢ Dia: Segundas-feiras
  â€¢ HorÃ¡rio: 02:00 - 04:00 UTC
  â€¢ Impacto: Queries podem estar lentas durante atualizaÃ§Ã£o

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… GARANTIAS DE PERFORMANCE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

QUERIES SIMPLES (filtros em 1-2 colunas):
  â€¢ SLA: < 2 segundos
  â€¢ Exemplos: Vendas por seller, top 10 produtos

QUERIES COMPLEXAS (mÃºltiplos JOINs):
  â€¢ SLA: 5-30 segundos
  â€¢ Exemplos: AnÃ¡lise de churn, distribuiÃ§Ã£o geogrÃ¡fica

AGREGAÃ‡Ã•ES PRÃ‰-CALCULADAS:
  â€¢ SLA: < 1 segundo
  â€¢ BenefÃ­cio: Usadas em dashboards Power BI/Tableau

FULL TABLE SCANS:
  â€¢ SLA: Depende do filtro de data (minutos)
  â€¢ RecomendaÃ§Ã£o: Sempre filtrar por data

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“‹ RECOMENDAÃ‡Ã•ES DE USO:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PARA DASHBOARDS:
  âœ“ Use tabelas de agregaÃ§Ã£o (fact_order_daily, fact_order_category)
  âœ“ Use views (vw_customer_metrics, vw_seller_metrics)
  âœ“ Evite full scans, sempre use filtros de data

PARA ANÃLISES AD-HOC:
  âœ“ Use vw_fact_order_cleaned para dados com regras aplicadas
  âœ“ Use views de agregaÃ§Ã£o como base para suas anÃ¡lises
  âœ“ Considere usar Python/Spark para anÃ¡lises muito grandes

PARA OPERAÃ‡Ã•ES:
  âœ“ Monitorar tamanho das tabelas mensalmente
  âœ“ Validar qualidade de dados diariamente
  âœ“ Auditar queries lentas > 30 segundos

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸš€ TECNOLOGIAS UTILIZADAS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â€¢ Platform: Microsoft Fabric / Synapse
  â€¢ Storage: Delta Lake (Lakehouse)
  â€¢ Engine: Apache Spark
  â€¢ Format: Delta (ACID, particionado)
  â€¢ OtimizaÃ§Ãµes: Z-order, Predicate Pushdown, Column Pruning

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â“ SUPORTE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Em caso de problemas:
  1. Verificar status do Fabric/Synapse (https://status.fabric.microsoft.com)
  2. Verificar Ãºltimo tempo de atualizaÃ§Ã£o do schema
  3. Validar se hÃ¡ parcelaÃ§Ã£o de dados corrompida
  4. Contatar time de dados

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

print(sla_doc)

# ============================================================================
# RESUMO DA FASE 3
# ============================================================================
print("\n" + "="*80)
print("âœ… FASE 3 RESUMO - OTIMIZAÃ‡Ã•ES CONCLUÃDAS")
print("="*80)
print(f"""
OTIMIZAÃ‡Ã•ES IMPLEMENTADAS:
  âœ“ Ãndices lÃ³gicos em chaves estrangeiras
  âœ“ Z-order aplicado em FACT_ORDER
  âœ“ Tabelas de agregaÃ§Ã£o prÃ©-calculadas criadas
  âœ“ SLA de atualizaÃ§Ã£o documentado

TABELAS DE AGREGAÃ‡ÃƒO CRIADAS:
  âœ“ fact_order_daily      â†’ AgregaÃ§Ã£o por data
  âœ“ fact_order_category   â†’ AgregaÃ§Ã£o por categoria

PERFORMANCE ESPERADA:
  âœ“ Queries simples:       < 2 segundos
  âœ“ Queries complexas:     5-30 segundos
  âœ“ AgregaÃ§Ãµes prÃ©-calc:   < 1 segundo

PRÃ“XIMO PASSO:
  â†’ Schema pronto para conectar com Power BI / Tableau
  â†’ Dashboards podem ser criados usando as tabelas de agregaÃ§Ã£o
""")

print("="*80 + "\n")

# MARKDOWN ********************

# # ğŸ“Š ANÃLISE E RECOMENDAÃ‡Ã•ES - MODELO DIMENSIONAL
# 
# ## ğŸ” SUMÃRIO EXECUTIVO DA ANÃLISE
# 
# ### CaracterÃ­sticas Principais dos Dados:
# 
# #### ğŸ“ˆ Volume e Granularidade
# - **Pedidos por Cliente**: MÃ¡ximo 1 pedido | MÃ©dia 1.00 
#   - âš ï¸ **Insight**: Cada cliente tem apenas 1 pedido na base (sem clientes recorrentes)
#   - **ImplicaÃ§Ã£o**: Simplifica relacionamentos cliente-pedido (1:1)
# 
# - **Itens por Pedido**: MÃ¡ximo 21 itens | MÃ©dia 1.14 itens
#   - âœ… **PadrÃ£o**: Maioria dos pedidos tem 1 item
#   - **ImplicaÃ§Ã£o**: FACT_ORDER em nÃ­vel de item Ã© a granularidade correta
# 
# - **Pagamentos por Pedido**: MÃ¡ximo 29 pagamentos | MÃ©dia 1.04
#   - ğŸ”” **Interessante**: Alguns pedidos com mÃºltiplos pagamentos (parcelamentos ou ajustes)
#   - **ImplicaÃ§Ã£o**: Manter pagamentos em FACT_ORDER garante rastreamento completo
# 
# - **Reviews por Pedido**: MÃ¡ximo 2236 reviews | MÃ©dia por pedido com review
#   - âš ï¸ **Anomalia Detectada**: Um pedido tem 2236 reviews (verificar integridade de dados)
#   - **RecomendaÃ§Ã£o**: Investigar essa anomalia antes de usar nos relatÃ³rios
# 
# - **Sellers**: 3.095 sellers Ãºnicos
#   - âœ… **Bom**: Volume significativo para anÃ¡lise de distribuiÃ§Ã£o de vendas
# 
# #### ğŸ—‚ï¸ Estrutura Recomendada: STAR SCHEMA
# 
# **RazÃµes principais:**
# 1. âœ… Relacionamentos lineares (1 pedido â†’ 1 cliente, 1 payment_date)
# 2. âœ… Granularidade clara em nÃ­vel de item de pedido
# 3. âœ… MÃºltiplas mÃ©tricas por item (preÃ§o, frete, review)
# 4. âœ… FÃ¡cil agregar para anÃ¡lises em diferentes nÃ­veis
# 5. âœ… Performance otimizada para BI (menos JOINs)
# 
# ---
# 
# ## ğŸ—ï¸ ESTRUTURA FINAL RECOMENDADA
# 
# ### Fatos Identificadas:
# - **NÃ­vel de Granularidade**: Item de pedido (order_item_id)
# - **Volume esperado**: ~1.1 M registros (1 pedido mÃ©dio Ã— 1.14 itens Ã— ~99K pedidos)
# - **MÃ©tricas principais**: 
#   - Quantidade vendida
#   - PreÃ§o unitÃ¡rio
#   - Frete
#   - Score de review
#   
# ### DimensÃµes NecessÃ¡rias:
# 1. **DIM_CUSTOMER** (chave: customer_id)
# 2. **DIM_PRODUCT** (chave: product_id)  
# 3. **DIM_SELLER** (chave: seller_id)
# 4. **DIM_TIME** (chave: date_key)
# 
# ### Relacionamentos CrÃ­ticos:
# ```
# order_item_id â†’ customer_id (1:1) â†’ pedido Ãºnico por cliente
# order_item_id â†’ product_id (muitos:1)
# order_item_id â†’ seller_id (muitos:1)  
# order_item_id â†’ order_date (muitos:1)
# order_item_id â†’ payment_date (muitos:1)
# ```
# 
# ---
# 
# ## âš ï¸ ALERTAS E CONSIDERAÃ‡Ã•ES
# 
# ### 1. Anomalia em Reviews
# - Um pedido contÃ©m 2.236 reviews (valor extremo)
# - **AÃ§Ã£o**: Aplicar regra de negÃ³cio (ex: mÃ¡ximo 1 review por pedido por cliente)
# 
# ### 2. MÃºltiplos Pagamentos
# - 29 pagamentos em um pedido Ã© anÃ´malo (verificar parcelamentos)
# - **AÃ§Ã£o**: Validar se Ã© parcelamento legÃ­timo ou duplicaÃ§Ã£o
# 
# ### 3. Clientes NÃ£o-Recorrentes
# - Cada cliente tem apenas 1 pedido
# - **ImplicaÃ§Ã£o para BI**: 
#   - ImpossÃ­vel anÃ¡lise de retenÃ§Ã£o/churn
#   - MÃ©tricas RFM nÃ£o aplicÃ¡veis
#   - Foco em anÃ¡lise de produto e seller
# 
# ---
# 
# ## âœ… PRÃ“XIMOS PASSOS RECOMENDADOS
# 
# 1. **CriaÃ§Ã£o do STAR SCHEMA**
#    - Usar as 4 dimensÃµes propostas
#    - Tabela FACT_ORDER em nÃ­vel de item
#    - Ãndices em chaves estrangeiras
# 
# 2. **ValidaÃ§Ã£o de Dados**
#    - Investigar anomalia de 2.236 reviews
#    - Validar parcelamentos (29 pagamentos)
#    - Verificar integridade de relacionamentos
# 
# 3. **OtimizaÃ§Ã£o**
#    - Desnormalizar informaÃ§Ãµes de localizaÃ§Ã£o no DIM_CUSTOMER/DIM_SELLER
#    - Usar data warehouse patterns (SCD Type 1)
#    - Particionar FACT_ORDER por data para grandes volumes
# 
# 4. **AnÃ¡lises PossÃ­veis com este Modelo**
#    - âœ… AnÃ¡lise de produtos mais vendidos (por categoria, vendedor)
#    - âœ… Performance de sellers (volume, preÃ§o mÃ©dio, satisfaÃ§Ã£o)
#    - âœ… AnÃ¡lise geogrÃ¡fica (estado/cidade de clientes e sellers)
#    - âœ… TendÃªncias de vendas por perÃ­odo
#    - âœ… AnÃ¡lise de satisfaÃ§Ã£o (review_score) por produto/seller
#    - âœ… AnÃ¡lise de frete e custo de distribuiÃ§Ã£o
# 
# ---
